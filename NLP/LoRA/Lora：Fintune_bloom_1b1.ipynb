{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SparKgod1/Skills-and-Expertise/blob/main/Lora%EF%BC%9AFintune_bloom_1b1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QXIbLP6EgOzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e0731f-acfc-46d8-c57d-3096c2cf3f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Dependencies\n"
      ],
      "metadata": {
        "id": "01zIxuZX0mhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git\n",
        "# !pip install --upgrade peft\n",
        "# !pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "e7aYz5D70CCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7d66bb-9d6e-4bbc-bdb3-e1293bb166e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Pre-Trained Model\n"
      ],
      "metadata": {
        "id": "xUGgwzqK124J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "#loading model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    # \"bigscience/bloom-3b\",\n",
        "    \"bigscience/bloom-1b1\",\n",
        "    # \"bigscience/bloom-560m\",\n",
        "    # torch_dtype=torch.float16,\n",
        "    device_map='auto',\n",
        "    cache_dir='/content/drive/MyDrive/mdl'\n",
        ")\n",
        "\n",
        "#加载tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b1\")\n",
        "special_tokens = {'additional_special_tokens':['<|beginofutterance|>', '<|endofutterance|>']}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "# 设置结束标记字符串\n",
        "end_token_str = \"<|endofutterance|>\"\n",
        "\n",
        "# 将结束标记添加到tokenizer的词汇表中\n",
        "tokenizer.add_tokens([end_token_str])\n",
        "\n",
        "# 获取结束标记的token id\n",
        "end_token_id = tokenizer.convert_tokens_to_ids(end_token_str)\n",
        "\n",
        "# 在模型中设置结束标记的token id\n",
        "model.config.eos_token_id = end_token_id"
      ],
      "metadata": {
        "id": "34LkV4O514v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up LoRA\n"
      ],
      "metadata": {
        "id": "5bFZ2oKI2fsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "BXq6vYyb29yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Printing Trainable Parameter Difference"
      ],
      "metadata": {
        "id": "qsSod_or3Fax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = 0\n",
        "all_param = 0\n",
        "\n",
        "for _, param in peft_model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "print(f\"trainable params: {trainable_params}\")\n",
        "print(f\"all params: {all_param}\")\n",
        "print(f\"trainable: {100 * trainable_params / all_param:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ep1kImT3KJc",
        "outputId": "37c99c91-88e1-4fba-dcf3-cd17fe26d474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1179648\n",
            "all params: 1066493952\n",
            "trainable: 0.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset"
      ],
      "metadata": {
        "id": "BCii0BM441AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "json_file_path = '/content/drive/MyDrive/HC3_Chinese_ChatGPT.json'\n",
        "\n",
        "qa_dataset = load_dataset('json', data_files=json_file_path)\n",
        "\n",
        "print(qa_dataset)\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "GPbR9Yk132nU",
        "outputId": "8342e4b9-73d0-4dc0-cdc6-6d9a3265695d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['output', 'input', 'instruction'],\n",
            "        num_rows: 17925\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-Formatting\n"
      ],
      "metadata": {
        "id": "0qqx4lwA5x9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(context, question, answer):\n",
        "  prompt_template = f\"<|beginofutterance|>系统\\n{context}\\n<|endofutterance|>\\n<|beginofutterance|>用户\\n{question}\\n<|endofutterance|>\\n<|beginofutterance|>智能助手\\n{answer}<|endofutterance|>\\n\"\n",
        "  return prompt_template\n",
        "\n",
        "mapped_qa_dataset = qa_dataset.map(lambda samples: tokenizer(create_prompt(samples['instruction'], samples['input'], samples['output'])))"
      ],
      "metadata": {
        "id": "gpDfILFX6Tud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training our LoRA model\n"
      ],
      "metadata": {
        "id": "ytep61CF6lQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "trainer = transformers.Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=mapped_qa_dataset[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=100,\n",
        "        max_steps=1000,\n",
        "        # num_train_epochs=10,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=100,\n",
        "        output_dir='/content/drive/MyDrive/outputs_1',\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "torch.cuda.empty_cache()\n",
        "# peft_model.config.use_cache = False  # silence the warnings.\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "bVcKf0h16wL-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "36dce289-6620-403f-95e8-4beba5c2c25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 02:22, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.853300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.924600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.784700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.728400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.707000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.623200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.577100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.610200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.614100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.533900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=2.7956449127197267, metrics={'train_runtime': 144.871, 'train_samples_per_second': 6.903, 'train_steps_per_second': 6.903, 'total_flos': 557438661697536.0, 'train_loss': 2.7956449127197267, 'epoch': 0.06})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Locally\n"
      ],
      "metadata": {
        "id": "7ki4RGWm7UGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"BLOOM-1b1-LoRA_1800steps\"\n",
        "peft_model.save_pretrained(\"/content/drive/MyDrive/BLOOM1111111111\")"
      ],
      "metadata": {
        "id": "I-tQ5ng27ts0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge model"
      ],
      "metadata": {
        "id": "p8oASzJ57Buo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloom-1b1\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "# apply and merge adapter 1\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    \"/content/drive/MyDrive/BLOOM1111111111\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained('/content/drive/MyDrive/_1')"
      ],
      "metadata": {
        "id": "JCG0PJiIQVwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test"
      ],
      "metadata": {
        "id": "Ca3tWfa97qwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model_raw = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloom-1b1\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "model_finetuned = AutoModelForCausalLM.from_pretrained(\n",
        "    \"/content/drive/MyDrive/model_finetuened\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model_finetuned = model_finetuned.to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/tokenizer\")\n",
        "special_tokens = {'additional_special_tokens':['<|beginofutterance|>', '<|endofutterance|>']}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/tokenizer\")\n",
        "# 设置结束标记字符串\n",
        "end_token_str = \"<|endofutterance|>\"\n",
        "\n",
        "# 将结束标记添加到tokenizer的词汇表中\n",
        "tokenizer.add_tokens([end_token_str])\n",
        "\n",
        "# 获取结束标记的token id\n",
        "end_token_id = tokenizer.convert_tokens_to_ids(end_token_str)\n",
        "\n",
        "# 在模型中设置结束标记的token id\n",
        "model_finetuned.config.eos_token_id = end_token_id\n",
        "model_raw.config.eos_token_id = end_token_id\n"
      ],
      "metadata": {
        "id": "ZVxM7e8f71rW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc59cb39-aeca-4025-99b6-da100bbc2cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"请回答下面的问题\"\n",
        "question = \"哈工大有多少杰出人才\"\n",
        "batch = tokenizer(f\"\\n<|beginofutterance|>系统\\n{context}\\n<|endofutterance|>\\n<|beginofutterance|>用户\\n{question}\\n<|endofutterance|>\\n<|beginofutterance|>智能助手\\n\", return_tensors='pt', return_token_type_ids=False)\n",
        "batch = batch.to(\"cuda\")\n",
        "# 对原始预训练模型生成回答\n",
        "outputs_raw = model_raw.generate(**batch, max_length=100, eos_token_id=tokenizer.eos_token_id)\n",
        "answer_raw = tokenizer.decode(outputs_raw[0], skip_special_tokens=True)\n",
        "print(\"原始预训练模型回答:\", answer_raw)\n",
        "\n",
        "# 对微调模型生成回答\n",
        "outputs_finetuned = model_finetuned.generate(**batch, max_length=100, eos_token_id=tokenizer.eos_token_id)\n",
        "answer_finetuned = tokenizer.decode(outputs_finetuned[0], skip_special_tokens=True)\n",
        "print(\"微调模型回答:\", answer_finetuned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDAOe25HQxcs",
        "outputId": "e5800d94-4ecf-4e78-ac8f-49a686bc0db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "原始预训练模型回答: \n",
            "系统\n",
            "请回答下面的问题\n",
            "\n",
            "用户\n",
            "哈工大有多少杰出人才\n",
            "\n",
            "智能助手\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问一下\n",
            "问\n",
            "微调模型回答: \n",
            "系统\n",
            "请回答下面的问题\n",
            "\n",
            "用户\n",
            "哈工大有多少杰出人才\n",
            "\n",
            "智能助手\n",
            "哈工大是我国著名的高等院校，拥有众多的杰出人才。哈工大在人才培养方面具有独特的优势，在人才培养方面具有丰富的经验。哈工大在人才培养方面具有独特的优势，在人才培养方面具有丰富的经验。在哈工大，学生可以获得系统的教育，包括基础教育、专业教育、职业教育、\n"
          ]
        }
      ]
    }
  ]
}